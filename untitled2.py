# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ykzWkyNse5n8x7itM5Y_JdWEJDgua7w
"""

!pip install torch torchaudio torchvision transformers

import os, random, torch, torch.nn as nn, torch.nn.functional as F
import torchaudio
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from IPython.display import Audio, display

from google.colab import drive
drive.mount('/content/drive')

!pip install gdown
!gdown --id 1RyMcp-ESo4LYZr9VOphh7gTuzzdvz9Ia

class TrainAudioSpectrogramDataset(Dataset):
    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0):
        self.root_dir = root_dir
        self.categories = categories
        self.max_frames = max_frames
        self.file_list = []
        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}

        for cat_name in self.categories:
            cat_dir = os.path.join(root_dir, cat_name)
            files_in_cat = [os.path.join(cat_dir, f) for f in os.listdir(cat_dir) if f.endswith(".wav")]
            num_to_sample = int(len(files_in_cat) * fraction)
            sampled_files = random.sample(files_in_cat, num_to_sample)
            label_idx = self.class_to_idx[cat_name]
            self.file_list.extend([(file_path, label_idx) for file_path in sampled_files])

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, idx):
        path, label = self.file_list[idx]
        wav, sr = torchaudio.load(path)
        if wav.size(0) > 1:
            wav = wav.mean(dim=0, keepdim=True)

        mel_spec = torchaudio.transforms.MelSpectrogram(
            sample_rate=sr, n_fft=1024, hop_length=256, n_mels=128 # Changed n_mels to 128 for consistency
        )(wav)
        log_spec = torch.log1p(mel_spec)

        _, _, n_frames = log_spec.shape
        if n_frames < self.max_frames:
            pad = self.max_frames - n_frames
            log_spec = F.pad(log_spec, (0, pad))
        else:
            log_spec = log_spec[:, :, :self.max_frames]

        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()
        return log_spec, label_vec

class CGAN_Generator(nn.Module):
    def __init__(self, latent_dim, num_categories, spec_shape=(128, 512)):
        super().__init__()
        self.latent_dim = latent_dim
        self.num_categories = num_categories
        self.spec_shape = spec_shape
        self.fc = nn.Linear(latent_dim + num_categories, 256 * 8 * 32)
        self.unflatten_shape = (256, 8, 32)
        self.net = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.BatchNorm2d(32), nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.ReLU()
        )

    def forward(self, z, y):
        h = torch.cat([z, y], dim=1)
        h = self.fc(h).view(-1, *self.unflatten_shape)
        return self.net(h)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import spectral_norm

class CGAN_Discriminator(nn.Module):
    def __init__(self, num_categories):
        super(CGAN_Discriminator, self).__init__()
        self.num_categories = num_categories

        # Match the input spectrogram shape (1, 128, 512)
        self.label_embedding = nn.Linear(num_categories, 128 * 512)

        # Convolutional feature extractor
        self.model = nn.Sequential(
            spectral_norm(nn.Conv2d(2, 64, 4, 2, 1)),
            nn.LeakyReLU(0.2, inplace=True),

            spectral_norm(nn.Conv2d(64, 128, 4, 2, 1)),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            spectral_norm(nn.Conv2d(128, 256, 4, 2, 1)),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            spectral_norm(nn.Conv2d(256, 512, 4, 2, 1)),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
        )

        # Dynamically created later (so it adapts to any input size)
        self.final_fc = None

    def forward(self, x, labels):
        # Embed category labels and reshape to match spectrogram size
        label_map = self.label_embedding(labels).view(-1, 1, 128, 512)

        # Concatenate along channel dimension
        d_in = torch.cat([x, label_map], dim=1)

        # Pass through conv layers
        out = self.model(d_in)
        out = out.view(out.size(0), -1)

        # Dynamically initialize final linear layer
        if self.final_fc is None:
            self.final_fc = spectral_norm(nn.Linear(out.size(1), 1)).to(out.device)

        return self.final_fc(out)

def generate_audio_gan(generator, category_idx, num_samples, device, sample_rate=22050):
    generator.eval()
    y = F.one_hot(torch.tensor([category_idx]), num_classes=generator.num_categories).float().to(device)
    z = torch.randn(num_samples, generator.latent_dim, device=device)

    with torch.no_grad():
        log_spec_gen = generator(z, y)

    # Convert log-mel spectrogram to mel spectrogram
    spec_gen = torch.expm1(log_spec_gen).squeeze(1)

    # ✅ Add noise + clamp to stabilize InverseMelScale
    spec_gen = spec_gen + 1e-4 * torch.rand_like(spec_gen)   # Add small random noise
    spec_gen = torch.clamp(spec_gen, min=1e-3, max=10.0)     # Prevent zeros or huge spikes

    # Define transforms
    inverse_mel = torchaudio.transforms.InverseMelScale(
        n_stft=1024 // 2 + 1, n_mels=128, sample_rate=sample_rate
    ).to(device)

    griffin = torchaudio.transforms.GriffinLim(
        n_fft=1024, hop_length=256, win_length=1024, n_iter=32
    ).to(device)

    # ✅ Handle invalid or degenerate mel spectrograms
    if (not torch.isfinite(spec_gen).all()) or torch.allclose(spec_gen, torch.zeros_like(spec_gen)):
        print("⚠️ Invalid mel spectrogram — returning silence.")
        return torch.zeros(num_samples, 1, int(sample_rate * 0.1)).cpu()

    try:
        linear_spec = inverse_mel(spec_gen)
        wav = griffin(linear_spec).cpu()

        # ✅ Sanity clamp to [-1, 1] range
        wav = torch.clamp(wav, -1.0, 1.0)

    except Exception as e:
        print(f"⚠️ InverseMelScale failed — returning silence instead: {e}")
        wav = torch.zeros(num_samples, 1, int(sample_rate * 0.1)).cpu()

    return wav


def save_and_play(wav, sample_rate, filename):
    if wav.dim() > 2:
        wav = wav.squeeze(0)
    torchaudio.save(filename, wav, sample_rate)
    print(f"✅ Saved to {filename}")
    display(Audio(data=wav.numpy(), rate=sample_rate))

def train_gan(generator, discriminator, dataloader, device, categories, epochs, lr, latent_dim):
    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))
    criterion = nn.BCEWithLogitsLoss()
    os.makedirs("gan_generated_audio", exist_ok=True)
    os.makedirs("gan_spectrogram_plots", exist_ok=True)

    for epoch in range(1, epochs + 1):
        loop = tqdm(dataloader, desc=f"Epoch {epoch}/{epochs}", leave=True)
        for real_specs, labels in loop:
            real_specs, labels = real_specs.to(device), labels.to(device)
            bs = real_specs.size(0)
            real_y = torch.full((bs, 1), 0.9, device=device)  # label smoothing
            fake_y = torch.full((bs, 1), 0.0, device=device)

            # Train D
            optimizer_D.zero_grad()
            loss_D_real = criterion(discriminator(real_specs, labels), real_y)
            fake_specs = generator(torch.randn(bs, latent_dim, device=device), labels)
            loss_D_fake = criterion(discriminator(fake_specs.detach(), labels), fake_y)
            loss_D = loss_D_real + loss_D_fake
            loss_D.backward(); optimizer_D.step()

            # Train G
            optimizer_G.zero_grad()
            loss_G = criterion(discriminator(fake_specs, labels), real_y)
            loss_G.backward(); optimizer_G.step()

            loop.set_postfix(loss_D=loss_D.item(), loss_G=loss_G.item())

        # Generate & visualize only after a few epochs to allow generator to learn
        if epoch >= 50 and epoch % 10 == 0: # Start generating from epoch 50, and every 10 epochs after that
            print(f"\n--- Generating Samples for Epoch {epoch} ---")
            generator.eval()
            fig, axes = plt.subplots(1, len(categories), figsize=(4*len(categories), 4))
            if len(categories) == 1: axes = [axes]
            for i, cat in enumerate(categories):
                y_cond = F.one_hot(torch.tensor([i]), num_classes=generator.num_categories).float().to(device)
                z = torch.randn(1, generator.latent_dim).to(device)
                with torch.no_grad(): spec = generator(z, y_cond).squeeze().cpu().numpy()
                axes[i].imshow(spec, aspect='auto', origin='lower', cmap='viridis')
                axes[i].set_title(f'{cat} (Epoch {epoch})'); axes[i].axis('off')
            plt.tight_layout(); plt.savefig(f'gan_spectrogram_plots/epoch_{epoch:03d}.png'); plt.show(); plt.close(fig)

            for i, cat in enumerate(categories):
                wav = generate_audio_gan(generator, i, 1, device)
                save_and_play(wav, 22050, f"gan_generated_audio/{cat}_ep{epoch}.wav")
            generator.train()
            print("--- End of Sample Generation ---\n")

import zipfile

zip_path = "/content/drive/MyDrive/audio_classification_with_test.zip"
extract_path = "/content/drive/MyDrive/organized_dataset"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("✅ Dataset extracted successfully!")

if __name__ == '__main__':
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    LATENT_DIM, EPOCHS, BATCH_SIZE, LR = 100, 200, 32, 2e-4
    BASE_PATH = 'drive/MyDrive/organized_dataset/'
    TRAIN_PATH = os.path.join(BASE_PATH, 'train')
    train_categories = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))])
    NUM_CATEGORIES = len(train_categories)
    print(f"Using device: {DEVICE}\nFound {NUM_CATEGORIES} categories: {train_categories}")

    train_dataset = TrainAudioSpectrogramDataset(root_dir=TRAIN_PATH, categories=train_categories)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
    generator, discriminator = CGAN_Generator(LATENT_DIM, NUM_CATEGORIES).to(DEVICE), CGAN_Discriminator(NUM_CATEGORIES).to(DEVICE)

    train_gan(generator, discriminator, train_loader, DEVICE, train_categories, EPOCHS, LR, LATENT_DIM)